<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Neural Network Rationale</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>
<!DOCTYPE html>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
  <div class="navbar-header">
  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
    <i class="fa fa-bars fa-lg fa-inverse"></i>
  </button>
  <a class="navbar-brand" href="index.html">Actuarial Analytics</a>
  </div>
  
  <div id="navbar" class="collapse navbar-collapse">
  <ul class="nav navbar-nav">
  <li><a href="AWS_tricks.html">AWS tricks</a></li>
  <li class="dropdown">
      <a class="dropdown-toggle" data-toggle="dropdown">Deep Learning <b class="caret"></b></a>
      <ul class="dropdown-menu" role="menu">
        <li><a href="http://image-classifier.actuarial-analytics.ca/">Image Classifier</a></li>
        <li><a href="https://jeremiedb.github.io/R_Quebec/">Presentation</a></li>
        <li><a href="deep_learning_rationale.html">Rationale</a></li>
      </ul>
  </li>
  
    <li class="dropdown">
  <a class="dropdown-toggle" data-toggle="dropdown">Kaggle <b class="caret"></b></a>
  <ul class="dropdown-menu" role="menu">
    <li><a href="kaggle_functions.html">Utility functions</a></li>
    <li><a href="kaggle_BNP.html">BNP</a></li>
    <li><a href="kaggle_MNIST.html">MNIST</a></li>
  </ul>
  </li>
  
  <li class="dropdown">
  <a class="dropdown-toggle" data-toggle="dropdown">Reserving <b class="caret"></b></a>
  <ul class="dropdown-menu" role="menu">
    <li><a href="reserving_comm_auto_new.html">Comm. auto</a></li>
    <li><a href="reserving_pers_auto_new.html">Pers. auto</a></li>
  </ul>
  </li>
  
  <li><a href="http://modeling.actuarial-analytics.ca/">Modeling</a></li>
  <li><a href="http://curve-fit.actuarial-analytics.ca/">Curve fitting</a></li>
  
  </ul>
  
  <ul class="nav navbar-nav navbar-right">
    <li class=navbar-right><a href="mailto:nimus44@gmail.com?Subject=Actuarial%20Analytics" ><i class="fa fa-envelope fa-lg"></i></a></li>
    <li class=navbar-right><a href="https://github.com/jeremiedb" ><i class="fa fa-github fa-lg"></i></a></li>
  </ul>
  
  </div><!--/.nav-collapse -->
  </div><!--/.container -->
  </nav><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Neural Network Rationale</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#neural-net-flow">Neural net flow</a></li>
<li><a href="#back-propagation">Back-propagation</a><ul>
<li><a href="#detailed-calculations">Detailed calculations</a></li>
<li><a href="#derivative-of-the-softmax-and-loss-functions">Derivative of the softmax and loss functions</a></li>
</ul></li>
</ul>
</div>

<hr />
<p> </p>
<div id="neural-net-flow" class="section level1">
<h1>Neural net flow</h1>
<p><span class="math display">\[I_1 \cdot W_1 = H_1 \to Act\_fun_1 \to I_2 \cdot W_2 = H_2 \to Softmax \to I_3 \to Loss\_function \to L\]</span></p>
<p><span class="math display">\[I_1[n, x]= input\ data\ including\ a\ bias\ variable\]</span></p>
<p><span class="math display">\[W_1[x, h_1] = weighting\ parameters\ for\ first\ hidden\ layer\]</span></p>
<p><span class="math display">\[H_1[n, h_1] = first\ hidden\ layer\ input\]</span></p>
<p><span class="math display">\[I_2[n, h_1]= input\ for\ second\ hidden\ layer = Activation\_function_1(H_1)\]</span></p>
<p><span class="math display">\[W_2[h1, h2] = weighting\ parameters\ for\ first\ hidden\ layer\]</span></p>
<p><span class="math display">\[H_2[n, h2] = second\ hidden\ layer\ input\]</span></p>
<p><span class="math display">\[I_3[n, h_2] = Softmax(H_2) = \frac{exp(H_2)}{rowSums(H_2)}\]</span></p>
<p><span class="math display">\[L[n, h_2] = loss\ metric\]</span></p>
<p> </p>
</div>
<div id="back-propagation" class="section level1">
<h1>Back-propagation</h1>
<p>Compute successively <span class="math inline">\(\frac{\partial Loss}{\partial W_i}\)</span> starting from the last layer</p>
<p><span class="math display">\[\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial I_3} \times \frac{\partial I_3}{\partial H_2} \times \frac{\partial H_2}{\partial W_2}\]</span></p>
<p><span class="math display">\[\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial I_3} \times \frac{\partial I_3}{\partial H_2} \times \frac{\partial H_2}{\partial I_2} \times \frac{\partial I_2}{\partial H_1} \times  \frac{\partial H_1}{\partial W_1}\]</span></p>
<div id="detailed-calculations" class="section level2">
<h2>Detailed calculations</h2>
<p>Below details the computation for a data with 3 samples and 2 variables for <span class="math inline">\(\frac{\partial L}{\partial W_2}\)</span> and assuming a squared error loss function and no transformation of the last hidden layer.</p>
<p><span class="math display">\[I =  \begin{bmatrix}
I_{11} &amp; I_{12}\\ 
I_{21} &amp; I_{22}\\ 
I_{31} &amp; I_{32}\\ 
\end{bmatrix}\]</span></p>
<p><span class="math display">\[W =  \begin{bmatrix}
W_1 \\ 
W_2\\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[H =  \begin{bmatrix}
H_1 \\ 
H_2\\
H_3\\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[T = target\ values = \begin{bmatrix}
T_1 \\ 
T_2\\
T_3\\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[L =  \begin{bmatrix}
L_1 \\ 
L_2\\
L_3\\
\end{bmatrix} = (H-T)^2\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial H} = 2(H-T) = 
2 \times \begin{bmatrix}
H_1-T_1 \\ 
H_2-T_2\\
H_3-T_3\\
\end{bmatrix} = 
\begin{bmatrix}
dH_1\\ 
dH_2\\
dH_3\\
\end{bmatrix}
\]</span></p>
<p>The above <span class="math inline">\(\frac{\partial L}{\partial H}\)</span> matrix dimension is [n, h]. Here h=1, that is only one hidden neuron in the layer.</p>
<p>Next step is to compute <span class="math inline">\(\frac{\partial H}{\partial W}\)</span></p>
<p><span class="math display">\[
H = IW = 
\begin{bmatrix}
I_{11}W_1+I_{12}W_2 \\
I_{21}W_1+I_{22}W_2 \\
I_{31}W_1+I_{32}W_2 \\
\end{bmatrix}
\]</span></p>
<p>Above matrix dimension is [n, h].</p>
<p>Next calculation is more tricky. It will results in a [x, n] matrix, x being the number of variables in the input data, which is 2 in this case. The result is simply the transpose of the I matrix, implying that it doesn’t depend on the number of hidden neurons (h), which makes sense since each neuron is activated by the same vector of variables.</p>
<p><span class="math display">\[
\frac{\partial H}{\partial W} = t(I) = 
\begin{bmatrix}
I_{11} &amp; I_{21} &amp; I_{31}\\
I_{12} &amp; I_{22} &amp; I_{32}\\
\end{bmatrix}
\]</span></p>
</div>
<div id="derivative-of-the-softmax-and-loss-functions" class="section level2">
<h2>Derivative of the softmax and loss functions</h2>
<p>Previous calculations assumed a simple loss squared error loss function applicable in a regression scenario.</p>
<p>For classification, calculation may appear less straightforward. The target is a [n, K] matrix, where K is the number of classes. Each row contains 0 for all K columns except for the observed class where <span class="math inline">\(T_{ij}=1\)</span>.</p>
<p>Loss function used is <span class="math inline">\(\sum_{i,j}^{n,k} (t_{ij} \times log(I_{ij)})\)</span>.</p>
<p><span class="math display">\[
\frac{\partial L}{\partial I} = \frac{T}{I}
\]</span></p>
<p>The above being a [n, k] matrix.</p>
<p>Remaining step if the calculation of <span class="math inline">\(\frac{\partial I}{\partial H}\)</span>, where <span class="math inline">\(I = \frac{exp(H)}{rowSums(H)}\)</span>.</p>
<p><span class="math display">\[
\frac{\partial I}{\partial H} = \frac{\partial \frac{exp(H_{i1})}{exp(H_{i1}+exp(H_{i2})+exp(H_{i3})}}{\partial H_{ij}} = 
I_{ij}-I{ij}^2
\]</span></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
