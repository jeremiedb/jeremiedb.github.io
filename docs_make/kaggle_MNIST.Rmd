---
title: "Kaggle: MNIST"
output: 
  html_document:
   toc: true
   toc_depth: 2
   number_section: false
   self_contained: false
---

\ 

#Data import and preparation


```

train <- read.csv('data/train.csv', header=TRUE)
test <- read.csv('data/test.csv', header=TRUE)

set.seed(123123)
eval_rows<- sample(1:nrow(train), size = round(0.1*nrow(train),0), replace = F)
eval<- train[eval_rows,]
train<- train[-eval_rows,]


train <- data.matrix(train)
eval <- data.matrix(eval)
test <- data.matrix(test)

train_x <- train[,-1]
train_labels <- train[,1]

eval_x <- eval[,-1]
eval_labels <- eval[,1]

### Normalise pixels into 0-1 range
train_x <- t(train_x/255)
eval_x <- t(eval_x/255)
test <- t(test/255)


################################################################
#### Convert data to arrays
train.array <- train_x
dim(train.array) <- c(28, 28, 1, ncol(train_x))
eval.array <- eval_x
dim(eval.array) <- c(28, 28, 1, ncol(eval_x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))

```


#Deep learning with MXNet

##Define the model

```

# input
data <- mx.symbol.Variable('data')
# first conv
conv1 <- mx.symbol.Convolution(data=data, kernel=c(3,3), num_filter=32)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="relu")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2,2), stride=c(2,2))
# second conv
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=32)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="relu")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2,2), stride=c(2,2))
# first fullc
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="relu")
# second fullc
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
# loss
convnet <- mx.symbol.SoftmaxOutput(data=fc2)


```


##Prepare an executor to visualise fitted model on single images

```

devices <- mx.cpu()


### To visualize convulation layer outputs
out <- mx.symbol.Group(c(tanh1, pool1, tanh2, pool2, tanh3, convnet))
# Create an executor
executor <- mx.simple.bind(symbol=out, data=dim(test.array), ctx=devices)

```


##Train the model

```

mx.set.seed(0)

model_convnet <- mx.model.FeedForward.create(convnet, X=train.array, y=train_labels, eval.data = list(data=eval.array, label=eval_labels), 
                                             ctx=devices, num.round=10, array.batch.size=100,
                                             learning.rate=0.05, momentum=0.9, wd=0.00001,
                                             eval.metric=mx.metric.accuracy, 
                                             initializer=mx.init.uniform(0.1),
                                             epoch.end.callback=mx.callback.log.train.metric(10)) 
                                             

```

##Make prediction

```
pred_prob<- t(predict(model_convnet, test.array))
pred_labels<- max.col(pred_prob)-1
submit<- data.frame(ImageId=1:length(pred_labels), Label=pred_labels)
write.csv(submit, file = "submit/mxnet_convnet_full.csv", row.names = F)

```

##Connecting layer options

fully_connected_layer:  mx.symbol.FullyConnected(data=input_layer,   num_hidden=10, no_bias=F, name="Optional Layer Name")

convolution_layer <- mx.symbol.Convolution(data=input_layer, kernel=c(3,3), num_filter=32, stride=c(y,x), dilate=c(y,x), pad=c(y,x), num_filter = integer)


##Activation layer options

activation <- mx.symbol.Activation(data=connected_layer, act_type=c("relu","tanh","sigmoid","softrelu")

activation <- mx.symbol.LeakyReLU(data=connected_layer, act_type=c("elu","leaky","prelu","rrelu"), slope=0.25 (elu and leaky), lower_bound=0.125 (relu), upper_bound=0.334 (relu))

##Output layer options

softmax <- mx.symbol.SoftmaxOutput(data=input)
linear <- mx.symbol.LinearRegressionOutput(data=input)
logistic <- mx.symbol.LogisticRegressionOutput(data=input)
mean_abs_error<- mx.symbol.MAERegressionOutput(data=input)

