---
title: "Neural Network Rationale"
output: 
  html_document: 
    toc: true

---

```{r setup, include=FALSE}

require("dplyr")
require("tidyr")
require("ggplot2")
require("plotly")
require("Matrix")
require("DiagrammeR")

```

***

\ 

#Neural net flow


$$I_1 \cdot W_1 = H_1 \to Act\_fun_1 \to I_2 \cdot W_2 = H_2 \to Softmax \to I_3 \to Loss\_function \to L$$

$$I_1[n, x]= input\ data\ including\ a\ bias\ variable$$

$$W_1[x, h_1] = weighting\ parameters\ for\ first\ hidden\ layer$$

$$H_1[n, h_1] = first\ hidden\ layer\ input$$

$$I_2[n, h_1]= input\ for\ second\ hidden\ layer = Activation\_function_1(H_1)$$

$$W_2[h1, h2] = weighting\ parameters\ for\ first\ hidden\ layer$$

$$H_2[n, h2] = second\ hidden\ layer\ input$$

$$I_3[n, h_2] = Softmax(H_2) = \frac{exp(H_2)}{rowSums(H_2)}$$

$$L[n, h_2] = loss\ metric$$



```{r, echo=FALSE}
```

\ 

#Back-propagation

Compute successively $\frac{\partial Loss}{\partial W_i}$ starting from the last layer


$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial I_3} \times \frac{\partial I_3}{\partial H_2} \times \frac{\partial H_2}{\partial W_2}$$

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial I_3} \times \frac{\partial I_3}{\partial H_2} \times \frac{\partial H_2}{\partial I_2} \times \frac{\partial I_2}{\partial H_1} \times  \frac{\partial H_1}{\partial W_1}$$


##Detailed calculations

Below details the computation for a data with 3 samples and 2 variables for $\frac{\partial L}{\partial W_2}$ and assuming a squared error loss function and no transformation of the last hidden layer. 


$$I =  \begin{bmatrix}
I_{11} & I_{12}\\ 
I_{21} & I_{22}\\ 
I_{31} & I_{32}\\ 
\end{bmatrix}$$

$$W =  \begin{bmatrix}
W_1 \\ 
W_2\\
\end{bmatrix}$$

$$H =  \begin{bmatrix}
H_1 \\ 
H_2\\
H_3\\
\end{bmatrix}$$

$$T = target\ values = \begin{bmatrix}
T_1 \\ 
T_2\\
T_3\\
\end{bmatrix}$$

$$L =  \begin{bmatrix}
L_1 \\ 
L_2\\
L_3\\
\end{bmatrix} = (H-T)^2$$


$$
\frac{\partial L}{\partial H} = 2(H-T) = 
2 \times \begin{bmatrix}
H_1-T_1 \\ 
H_2-T_2\\
H_3-T_3\\
\end{bmatrix} = 
\begin{bmatrix}
dH_1\\ 
dH_2\\
dH_3\\
\end{bmatrix}
$$

The above $\frac{\partial L}{\partial H}$ matrix dimension is [n, h]. Here h=1, that is only one hidden neuron in the layer. 

Next step is to compute $\frac{\partial H}{\partial W}$

$$
H = IW = 
\begin{bmatrix}
I_{11}W_1+I_{12}W_2 \\
I_{21}W_1+I_{22}W_2 \\
I_{31}W_1+I_{32}W_2 \\
\end{bmatrix}
$$

Above matrix dimension is [n, h]. 

Next calculation is more tricky. It will results in a [x, n] matrix, x being the number of variables in the input data, which is 2 in this case. The result is simply the transpose of the I matrix, implying that it doesn't depend on the number of hidden neurons (h), which makes sense since each neuron is activated by the same vector of variables. 

$$
\frac{\partial H}{\partial W} = t(I) = 
\begin{bmatrix}
I_{11} & I_{21} & I_{31}\\
I_{12} & I_{22} & I_{32}\\
\end{bmatrix}
$$

##Derivative of the softmax and loss functions

Previous calculations assumed a simple loss squared error loss function applicable in a regression scenario. 

For classification, calculation may appear less straightforward. The target is a [n, K] matrix, where K is the number of classes. Each row contains 0 for all K columns except for the observed class where $T_{ij}=1$. 

Loss function used is $\sum_{i,j}^{n,k} (t_{ij} \times log(I_{ij)})$. 

$$
\frac{\partial L}{\partial I} = \frac{T}{I}
$$

The above being a [n, k] matrix. 

Remaining step if the calculation of $\frac{\partial I}{\partial H}$, where $I = \frac{exp(H)}{rowSums(H)}$. 


$$
\frac{\partial I}{\partial H} = \frac{\partial \frac{exp(H_{i1})}{exp(H_{i1}+exp(H_{i2})+exp(H_{i3})}}{\partial H_{ij}} = 
I_{ij}-I{ij}^2
$$